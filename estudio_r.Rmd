---
title: "MI ESTUDIO SOBRE R"
output: html_notebook
---
### Valores NA (missing values) y cómo rellenarlos
Partimos de uno de los datasets de pruebas que hay en RStudio.
```{r}
#library(help="datasets") #lista completa (cogeremos el dataset llamado attenu, por ejemplo)
data(attenu)
attach(attenu)
help(attenu)
boxplot(attenu, cex.axis=0.5) 
```
Vemos que la variable dist es asimetrica y tiene outliers.
Vamos que las variables mag y accel son leptocúrticas (apretadas en torno a la media)

```{r}
summary(attenu)
```




### Matriz de parejas de features (CORRELACIONES)
Hacemos una análisis exploratorio de las correlaciones.
Matriz de correlaciones (entre variables) y funcion de densidad de probabilidad (univariable):
```{r}
library(car)
scatterplotMatrix(x=attenu)
```
O también construirla usando una función personalizada por nosotros:
```{r}
pairs(attenu, gap=0, lower.panel = panel.smooth, upper.panel = function(x,y){
  panel.smooth(x,y)
  par(usr=c(0,1,0,1))
  correlacion <- cor(x,y,use="complete.obs")
  text(0.6,0.7,col="blue", cex=1.2, round(correlacion, digits=2))
}  )
```
Es importante ver si hay alguna correlación fácilmente visible (lineal...).

IDs de los elementos que tienen valores NA en cualquiera de sus features (columnas):
```{r}
which(is.na(attenu$event))
which(is.na(attenu$mag))
which(is.na(attenu$station))
which(is.na(attenu$dist))
which(is.na(attenu$accel))
```
Vemos que sólo la variable station tiene valores NA.
También se pueden coger sólo las filas en las que todas las features están rellenas (sin ningún valor NA):
```{r}
attenu[complete.cases(attenu),]
```
Si queremos ELIMINAR aquellas filas (margin=1 significa ROWS) que tengan más de un valor NA:
1. Aplicar la función SUM sobre filas, contando el número de elementos NA.
2. Si hay uno o más, saco sus índices (which)
3. Creo un vector con esos índices y se lo resto al dataset attenu.

```{r}
filas_con_na <- apply(attenu, MARGIN = 1, FUN = function(x){ sum(is.na(x)) >=1})
indices_de_filas_con_na <- which(filas_con_na)
datos_sinhuecos=attenu[ -c( indices_de_filas_con_na ) , ]
```

Comprobamos que ya no hay HUECOS (con algún valor NA):
```{r}
attenu[!complete.cases(datos_sinhuecos),]
```
### ANALISIS DE COMPONENTES PRINCIPALES (PCA)
Si tenemos muchas features, conviene reducirlas para que el análisis de correlaciones sea más sencillo.
Es MUY importante indicar que trabajamos con la matriz de correlaciones (en vez de la de covarianzas): cor=TRUE
Además, PCA sólo trabaja con features que sean NUMERICAS, así que debemos comprobar que todas las columnas (features) son numericas
```{r}
drops <- c("station")
datos_para_pca <- attenu[ , !(names(attenu) %in% drops)]
datos_sin_na <- na.omit(datos_para_pca) # limpiar los NA
sapply(datos_sin_na, class) #clase/modo de cada feature
sapply(datos_sin_na, typeof) #tipo de cada feature
pca_attenu <- princomp(datos_sin_na, cor=TRUE)  
pca_attenu
plot(pca_attenu)
summary(pca_attenu)
biplot(pca_attenu) #PC2 vs PC1

#Los pesos de cada individuo (fila) proyectado en las componentes (PC1, PC2...)
pca_pesos <- pca_attenu$scores
pca_pesos_pc1pc2 <- pca_pesos[, 1:2] #Pesos sólo en las componentes que más influyen en la varianza total
plot(pca_pesos_pc1pc2[,1], pca_pesos_pc1pc2[,2]) #misma gráfica que veiamos en el biplot
```
Vemos que la componente 1 tiene un peso del 54.41% sobre las varianzas; la 2 tiene 26.9%; etc.
Cogemos la PC1 y PC2, que suman un 81% de la influencia en la varianza total.

Vemos también los pesos que tienen las variables de entrada (features) dentro de las componentes compuestas (PC1, PC2...)., mirando el plot de **flechas**:

- PC1 (eje horizontal) está afectada por las variables: event, dist, mag y accel. (es decir, por todas las features de entrada, pero en proporción diferente).
- PC2 (eje vertical): está afectada por mag y accel, principalmente.


### CLUSTERING (habiendo hecho PCA antes)
Vamos a mostrar un subtipo de "clustering jerárquico" llamado *Agglomerative Nesting (AGNES)*.

Hay tres tipos de métodos de unión en clustering:

- *Single linkage*: según la MINIMA distancia entre dos elementos de clusters diferentes.
- *Complete linkage*: según la MAXIMA distancia entre dos elementos de clusters diferentes.
- *Average linkage*: según la distancia MEDIA entre dos elementos de clusters diferentes.

```{r}
library(cluster)
agnes_single <- agnes(x = datos_sin_na, method = "single")
plot(agnes_single) # Cuando veamos espacios entre las muestras, es la separacion entre clusters. Es DIFICIL de ver.
```
Con Single, vemos que hay 3 clusters (si height=50), fijándose en los huecos que hay en el gráfico "banner", que separan a los clusters.
```{r}
library(cluster)
agnes_complete <- agnes(x = datos_sin_na, method = "complete")
plot(agnes_complete) # Cuando veamos espacios entre las muestras, es la separacion entre clusters. Es DIFICIL de ver.
```
Con Complete, se ven claramente 3 clusters (si height=200).
```{r}
library(cluster)
agnes_average <- agnes(x = datos_sin_na, method = "average")
plot(agnes_average) # Cuando veamos espacios entre las muestras, es la separacion entre clusters. Es DIFICIL de ver.
```
Con Average, se ven 3 clusters (si height=100).
Donde más claro lo vemos es con el método "complete", así que creamos los *clusters* vistos con *COLORES*.
Esos colores los pintamos sobre los pesos que ya habiamos calculado para PCA (de las componentes PC1 y PC2).
```{r}
complete_3_clusters=cutree(agnes_complete,3)
plot(pca_pesos,col=complete_3_clusters)
```



### REGRESIÓN LINEAL SIMPLE
Link: <http://rpubs.com/joser/RegresionSimple>
Datos entrada:
```{r}
grasas_tabla <- read.table('http://verso.mat.uam.es/~joser.berrendero/datos/EdadPesoGrasas.txt', header = TRUE)
names(grasas_tabla)
pairs(grasas_tabla)
cor(grasas_tabla)
```
Vemos correlación (0.83) entre las variables edad y grasas, como era de esperar a simple vista. El gráfico muestra también la relación lineal (a ojo).

Recta de mínimos cuadrados:
Usamos un MODELO LINEAL (lm) para calcular una regresión lineal muy sencilla, de grasas respecto de la edad: $y =102.575+5.321*x$

O mejor aun:  $y = 102.575 (+/-29.6376) + 5.321(+/-0.7243) * x$

```{r}
regresion <- lm(grasas ~ edad, data = grasas_tabla) #grasas dependiente de la edad
summary(regresion)

plot(grasas_tabla$edad, grasas_tabla$grasas, xlab='Edad', ylab='Grasas')
abline(regresion)

plot(regresion)
```
Vemos que el R^2^ es 0.7012 --> Bondad de la recta de ajuste.

#### PREDICCION PARA NUEVAS ENTRADAS:
Usar la recta:
```{r}
nuevas.edades <- data.frame(edad = seq(30, 50))
prediccion_nueva<-predict(regresion, nuevas.edades)
prediccion_nueva
```

#### Explicación del modelo de regresión simple:
- La columna t-value es el estadístico t --> Cociente entre cada estimador y su error típico. Permite realizar los contrastes de hipótesis nula (que sean 0) --> H~0~:β~0~=0 y H~0~:β~1~=0 
- Los  p-valores aparecen en la columna Pr(>|t|) --> Probabilidad de superar el umbral --> Como son muy pequeños (menores que 0.01 ó 0.05, utilizados normalmente) ==> Se rechazan ambas hipótesis nulas.
- El estimador de la desviación típica (σ) de los errores aparece como Residual standard error y su valor en el ejemplo es 43.5
- Los intervalos de confianza para los parámetros se obtienen con el comando **confint**. El parámetro level permite elegir el nivel de confianza (por defecto es 0.95):
```{r}
confint(regresion)
confint(regresion, level = 0.90)
```

Los "intervalos de confianza para la respuesta media" y los "intervalos de predicción para la respuesta" se pueden obtener usando el comando predict. 
Por ejemplo, el siguiente código calcula y representa los dos tipos de intervalos para el rango de edades que va de 20 a 60 años (los de predicción en rojo):

```{r}
nuevas.edades <- data.frame(edad = seq(20, 60))
# Grafico de dispersion y recta
plot(grasas_tabla$edad, grasas_tabla$grasas, xlab='Edad', ylab='Grasas')
abline(regresion)

# Intervalos de confianza de la respuesta media:
# ic es una matriz con tres columnas: la primera es la prediccion, las otras dos son los extremos del intervalo
ic <- predict(regresion, nuevas.edades, interval = 'confidence')
lines(nuevas.edades$edad, ic[, 2], lty = 2, col = 'green')
lines(nuevas.edades$edad, ic[, 3], lty = 2, col = 'green')

# Intervalos de prediccion
ic <- predict(regresion, nuevas.edades, interval = 'prediction')
lines(nuevas.edades$edad, ic[, 2], lty = 2, col = 'red')
lines(nuevas.edades$edad, ic[, 3], lty = 2, col = 'red')
```
#### Análisis de la varianza:
```{r}
anova(regresion)
```

Diagnóstico del modelo:
```{r}
residuos <- rstandard(regresion)
valores.ajustados <- fitted(regresion)
plot(valores.ajustados, residuos)

#Prueba de normalidad (sencilla) para saber si la muestra tiene una distribucion NORMAL
qqnorm(residuos)
qqline(residuos)
```
Como tiene pocas muestras, podríamos haber pintado una t-Student encima, para poder comparar.

Otras pruebas de normalidad --> Kolmogorov-Smirnov  y Shapiro-Wilk.

#### Ejemplo interesante: 
```{r}
x <- 1:20
w <- 1 + sqrt(x)/2 #desviaciones estandar (pesos)
y <- x + w*rnorm(x)

dummy <-- data.frame(x=x, y=y)

fm <- lm(y~x, data=dummy) #MODELO simple linear regression
summary(fm)

fm1 <- lm(y~x, data=dummy, weight=1/w^2) #MODELO con weighted regression
summary(fm1)
```
Resto del análisis:
```{r}
attach(dummy) #hacemos visibles las columnas del dataframe, para trabajar directamente con sus nombres.
lrf <-lowess(x,y, f = 2/3, iter = 3, delta = 0.01 * diff(range(x))) #Funcion de regresion local no parametrica: LOWESS smoother (locally-weighted polynomial regression)
summary(lrf)
plot(x,y)
lines(x, lrf$y, col="blue") #pintamos la linea de la REGRESION LINEAL

abline(0,1, lty=3, col="magenta") #linea de regresion verdadera (intercept 0, slope=1) (linea de puntos)
abline(coef(fm), col="green") #linea de regresion sin pesos
abline(coef(fm1), col="red") #linea de regresion con pesos

detach()

plot( fitted(fm), resid(fm), xlab="Fitted values", ylab="Residuals", main="Residuals vs Fitted" ) #Analisis de heterodicidad
qqnorm( resid(fm), main="Residuals Rankit Plot" ) #Analisis de skewness, kurtosis y outliers (no muy util en este caso)
#rm( fm, fm1, lrf, x, dummy) #limpieza de variables
```
## Manual de CRAN-INTRO.ap.8.2
Lo básico:
```{r}
attach(faithful)

summary(eruptions) #Como he hecho attach, evito hacer esto: summary(faithful$eruptions)
fivenum(eruptions) #Pinta un "dibujo" hecho con numeros, que parece un histograma
stem(eruptions)
```
Dibujos:
```{r}
hist(eruptions,seq(from=1.6, to=5.2, by=0.2), probability = TRUE)
lines(density(eruptions, bw=0.1))
rug(eruptions)
```
La función de densidad (CDF) es:
```{r}
plot(ecdf(eruptions), do.points=FALSE, verticals = TRUE)
```
Si cogemos sólo aquellos con X>3:
```{r}
erupciones_largas<-eruptions[eruptions>3]
hist(erupciones_largas)
plot(ecdf(erupciones_largas), do.points=FALSE, verticals = TRUE)
x<-seq(3, 5.4, 0.01)

#Usando la distribucion NORMAL (norm)
lines(x, pnorm(x, mean = mean(erupciones_largas), sd=sqrt(var(erupciones_largas))), lty=3)  #linea de puntos(lty=3)
```

### PRUEBAS DE NORMALIDAD:

1. Q-Q (Quantile-Quantile)
```{r}
par(pty="s") #pinta la caja cuadrada (en vez de que sea rectangular por defecto)
qqnorm(erupciones_largas)
qqline(erupciones_largas)

#La COMPARAMOS con una t-Student de 5 grados de libertad:
puntos_t_student <- rt(250, df = 5)
qqnorm(puntos_t_student)
qqline(puntos_t_student)

#Pintamos el qqplot
qqplot(qt( ppoints(250), df=5 ), puntos_t_student, xlab = "QQplot para la densidad t-Student")
qqline(puntos_t_student)
```

2. Test Shapiro-Wilk:
```{r}
shapiro.test(erupciones_largas)
```

3. Test Kolmogorov-Smirnov:
```{r}
ks.test(erupciones_largas, "pnorm", mean=mean(erupciones_largas), sd=sqrt(var(erupciones_largas)))
```


### REGRESION LOGISTICA

```{r}
challenger <- read.table('http://verso.mat.uam.es/~joser.berrendero/datos/challenger.txt', header = TRUE)
table(challenger$defecto)

colores <- NULL
colores[challenger$defecto==0] <- 'green'
colores[challenger$defecto==1] <- 'red'
plot(challenger$temp, challenger$defecto, pch = 21, bg = colores, xlab = 'Temperatura', ylab = 'Probabilidad de defectos')
legend('bottomleft', c('No defecto', 'Si defecto'), pch = 21, col = c('green', 'red'))
```


### DISTRIBUCIONES DE PROBABILIDAD
Hay muchas: binomial (binom), Poisson (pois), normal (norm), exponencial (exp), t-Student (t), chi-cuadrado (chisq), F (f)...


### MAQUINAS DE VECTOR SOPORTE (SVM)
Busca el hiperplano que separe las observaciones según su clases (clasificación).
Buena explicación: <https://www.youtube.com/watch?v=ImhjKyc88x0>

Link de mi ejemplo: <http://rpubs.com/joser/svm>
```{r}
library(MASS)
library(e1071)
load(url('http://www.uam.es/joser.berrendero/datos/practica-svm-io.RData'))

# Mostramos 10 filas
head(breast.cancer2)
```

Gráficamente, pintamos la correlación entre las 2 features de entrada:
```{r}
# Prepara los datos
x <- cbind(breast.cancer2$x.smoothness, breast.cancer2$x.concavepoints)
head(x)
y <- breast.cancer2$y
head(y)
n0 <- sum(y==0)
n1 <- sum(y==1)
# Para que los graficos queden mas bonitos (rojo = maligno, verde = benigno)
colores <- c(rep('green',n0),rep('red',n1))
pchn <- 21

# Diagrama de dispersion
plot(x, pch = pchn, bg = colores, xlab='smoothness', ylab='concavepoints')
```

#### SVM lineal
Siendo la variable **y** un factor, queremos predecir usando el resto de features.

Para calcular la regla de clasificación SVM lineal con C=10, se usa la función svm del paquete e1071. 
El primer argumento y~. indica que la variable y (que debe ser necesariamente un factor) se desea predecir en términos del resto de variables del fichero. La sintaxis es similar a la que utilizaríamos para ajustar un modelo lineal o un modelo de regresión logística.
El segundo argumento indica el fichero en el que están las variables que vamos a usar. 
El argumento kernel corresponde al núcleo que representa el producto escalar que queremos utilizar. 
La opción linear corresponde a k(x,y)=x′y. 
El argumento cost determina la penalización que ponemos a los errores de clasificación. 
Con el fin de estimar la probabilidad de clasificar erróneamente una observación se puede utilizar validación cruzada, dividiendo la muestra en, por ejemplo, dos partes. Ello se consigue fijando cross=2. 
Finalmente, scale=FALSE se usa para usar los datos no estandarizados (por defecto, sí se estandarizan).
```{r}
C <- 10
svm.lineal <- svm(y~., data=breast.cancer2, kernel='linear', cost=C, cross=2, scale=FALSE) #Su type es C-classification porque y es un factor
summary(svm.lineal)

x.svm <- x[svm.lineal$index,]
w <- crossprod(x.svm, svm.lineal$coefs)
w0 <- svm.lineal$rho

plot(x, pch = pchn, bg = colores, xlab='smoothness', ylab='concavepoints')
abline(w0/w[2], -w[1]/w[2], lwd=2, col='blue')

#Estadisticas: precisión y cobertura
table(predict(svm.lineal), breast.cancer2$y, dnn=c("Prediction", "Actual"))
library(caret)
confusionMatrix(breast.cancer2$y, predict(svm.lineal))
```
Indica que hay 258 vectores soporte.
Usando validación cruzada con la muestra dividida en dos partes se estima una probabilidad de acierto en la clasificación de aproximadamente el 88%. Podemos cambiar el parámetro de penalización para ver si estos valores aumentan o disminuyen.

#### SVM cuadrático
```{r}
svm.cuadratico <- svm(formula = y~., data = breast.cancer2, kernel='polynomial', degree=2, gamma=1, coef0=1, cost=C, cross=10, scale=FALSE)
summary(svm.cuadratico)

#Estadisticas: precisión y cobertura
table(predict(svm.cuadratico), breast.cancer2$y, dnn=c("Prediction", "Actual"))
library(caret)
confusionMatrix(breast.cancer2$y, predict(svm.cuadratico))
```

#### SVM radial
```{r}
svm.radial <- svm(formula = y~., data = breast.cancer2, kernel='radial', degree=2, gamma=1, coef0=1, cost=C, cross=10, scale=FALSE)
summary(svm.radial)

#Estadisticas: precisión y cobertura
table(predict(svm.radial), breast.cancer2$y, dnn=c("Prediction", "Actual"))
library(caret)
confusionMatrix(breast.cancer2$y, predict(svm.radial))
```
Se ve que es mejor el kernel LINEAL (grado), pero debemos mirar el *accuracy* y *cobertura* --> Gana CUADRATICO, parece.



### ARBOLES DE DECISIÓN
Link: <https://www.youtube.com/watch?v=mgh4KdbYHv0>
Es un algoritmo de CLASIFICACIÓN supervisada. La variable dependiente predicha puede ser:

- Cuantitativa --> Árbol de REGRESIÓN
- Cualitativa --> Árbol de CLASIFICACIÓN

#### Cargar dataset
```{r}
library(rpart) #Algoritmo para árbol de decisión
library(rpart.plot) #Plot de árbol de decisión
library("C50") #paquete que tiene el dataset de entrada y el algoritmo (rpart)
data(churn) #DATASET de ejemplo sobre CHURN (clientes que abandonan una empresa)
```
#### Preparar set de datos, dividiendo en entrenamiento y test
```{r}
churn_juntos <- rbind(churnTest, churnTrain) #junta filas
head(churn_juntos, n=3L) #FEATURES disponibles (5000x20). Mostramos 3 filas
churn <- churn_juntos[, c(4,7,8,16,19,17,20)] #Usaremos solo algunas columnas (features)
names(churn) # nombres en ingles
names(churn) <- c("Tiene plan internacional", "Minutos al dia", "Llamadas al dia", "Minutos internacionales", "Reclamaciones", "Llamadas internacionales", "Cancelacion") #los renombro a nombres en espanhol

ind <- sample(2, nrow(churn), replace=TRUE, prob=c(0.6, 0.4)) #train (60%) y test (40%)
trainData <- churn[ind==1, ] #train
testData <- churn[ind==2, ] #test
```
#### Crear árbol de decisión
```{r}
ArbolRpart <- rpart(Cancelacion ~ ., method="class", data=trainData) #Formula: variable dependiente (Cancelacion) depende todas las otras
```

#### Gráficos
```{r}
print(ArbolRpart)                         
rpart.plot(ArbolRpart,extra=4)  # extra=4:probabilidad de observaciones por clase

printcp(ArbolRpart)             # estadísticas de resultados
plotcp(ArbolRpart)              # evolución del error a medida que se incrementan los nodos
```

```{r}
# Podado del árbol
pArbolRpart<- prune(ArbolRpart, cp= ArbolRpart$cptable[which.min(ArbolRpart$cptable[,"xerror"]),"CP"])
pArbolRpart<- prune(ArbolRpart, cp= 0.011111)
printcp(pArbolRpart)
```

#### Predice las cancelaciones en testData 
```{r}
# Validamos la capacidad de predicción del árbol con el fichero de validación
testPredRpart <- predict(ArbolRpart, newdata = testData, type = "class")

# Visualizamos una matriz de confusión
table(testPredRpart, testData$Cancelacion)
```

#### Estadística del modelo  
```{r}
# Calculamos el % de aciertos
sum(testPredRpart == testData$Cancelacion) / length(testData$Cancelacion)*100
```


### SVM (otro ejemplo)
Modelo SVM sobre datos de iris:
```{r}
data(iris)
attach(iris)

## classification mode
# default with factor response:
model <- svm(Species ~ ., data = iris)

# alternatively the traditional interface:
x <- subset(iris, select = -Species)
y <- Species
model <- svm(x, y) 

print(model)
summary(model)
```
Testeo con datos de entrenamiento (claro, el modelo está muy ajustado para ellos):
```{r}
# test with train data
pred <- predict(model, x)
# (same as:)
#pred <- fitted(model)

# Check accuracy:
table(pred, y)

# compute decision values and probabilities:
pred <- predict(model, x, decision.values = TRUE)
attr(pred, "decision.values")[1:4,]

# visualize (classes by color, SV by crosses):
distancias_escaladas <- cmdscale(dist(iris[,-5]))
plot(distancias_escaladas,
     col = as.integer(iris[,5]),
     pch = c("o","+")[1:150 %in% model$index + 1])

```
Modelo de REGRESION en 2 dimensiones:
```{r}
## try regression mode on two dimensions

# create data
x <- seq(0.1, 5, by = 0.05)
y <- log(x) + rnorm(x, sd = 0.2)

# estimate model and predict input values
m   <- svm(x, y)
summary(m)
new <- predict(m, x)

# visualize
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)

## density-estimation

```
Creamos otras features con distribución normal y modelamos:
```{r}
# create 2-dim. normal with rho=0:
X <- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)

# MODELO:
#traditional way:
#m <- svm(X, gamma = 0.1)
# formula interface:
m <- svm(~., data = X, gamma = 0.1)
# Otra forma:X <- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)

#m <- svm(~ a + b, gamma = 0.1)

summary(m)

# test:
newdata <- data.frame(a = c(0, 4), b = c(0, 4))
predict(m, newdata)

# visualize:
plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))
points(newdata, pch = "+", col = 2, cex = 5)

# weights: (example not particularly sensible)
i2 <- iris
levels(i2$Species)[3] <- "versicolor"
summary(i2$Species)
wts <- 100 / table(i2$Species)
wts
m <- svm(Species ~ ., data = i2, class.weights = wts)
```


### SVM (ejemplo perros y gatos)
Link: <https://www.youtube.com/watch?v=ImhjKyc88x0>
```{r}
library(e1071)
data(cats, package = 'MASS') # Features: sexo (sex), peso (bwt), peso_corazon (hwt)

#Datasets: train y test
ind <- sample(x=2, size = nrow(cats), replace = TRUE, prob = c(0.7,0.3)) #vector 1:144 de índices. Sus valores están en el rango entre 1:x; en este caso, los unos aparecerán un 70% y los doses un 30%.
testset <- cats[ind == 1,] #Coge las filas de CATS en aquellos indices que 'ind' toma valor 1
trainset <- cats[ind == 2,] #Coge las filas de CATS en aquellos indices que 'ind' toma valor 2

#Entrenamos el modelo
modelo_svm <- svm(Sex~., data=trainset, kernel = "radial")
prediccion <- predict(modelo_svm, newdata = testset[-1])

#Resultado y porcentaje de acierto
plot(modelo_svm, cats)
MC <- table(testset[,1], prediccion) #matriz de confusión
MC
acierto <- ( sum(diag(MC)) )/ (sum(MC))
acierto

```


###Otro ejemplo completo
Link: <https://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package/>

####Data split:
```{r}
# load the libraries
library(caret)
library(klaR)
# load the iris dataset
data(iris)
# define an 80%/20% train/test split of the dataset
split=0.80
trainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)
data_train <- iris[ trainIndex,]
data_test <- iris[-trainIndex,]
# train a naive bayes model
model <- NaiveBayes(Species~., data=data_train)
# make predictions
x_test <- data_test[,1:4]
y_test <- data_test[,5]
predictions <- predict(model, x_test)
# summarize results
confusionMatrix(predictions$class, y_test)
```
####Bootstrap:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="boot", number=100)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```
####k-fold Cross Validation:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")#, tuneGrid=grid
# summarize results
print(model)
```
####Repeated k-fold Cross Validation:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```
####Leave One Out Cross Validation:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="LOOCV")
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```

###Otro ejemplo SVM vs RPART:
```{r}
library(e1071)
library(rpart)

#DATOS:
data(Ozone, package="mlbench")

## Split en Datasets (train y test)
index     <- 1:nrow(Ozone)
tercio <- trunc(length(index)/3) #usamos un 30% para test
testindex <- sample(index, tercio, replace = FALSE, prob = NULL)
testset   <- na.omit(Ozone[testindex,]) #TEST Quita las filas que tengan ALGUN valor NA
trainset  <- na.omit(Ozone[-testindex,])#TRAIN Quita las filas que tengan ALGUN valor NA

#Columna TARGET
indice_target <- which( colnames(trainset) == "V4" )

## svm (MAQUINAS VECTOR SOPORTE) para REGRESIÓN NO LINEAL
svm.model <- svm(formula = V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001)
svm.pred  <- predict(svm.model, testset[,-indice_target])
svm_error <- crossprod(svm.pred - testset[,indice_target]) / length(testindex)
svm_error

## rpart (ARBOLES DE REGRESION) para REGRESIÓN NO LINEAL
rpart.model <- rpart(formula = V4 ~ ., data = trainset)
rpart.pred  <- predict(rpart.model, testset[,-indice_target])
rpart_error <- crossprod(rpart.pred - testset[,indice_target]) / length(testindex)
rpart_error
```

###ENSEMBLING
Link: <https://www.datacamp.com/community/tutorials/ensemble-r-machine-learning>

```{r}
library("SuperLearner")

library(MASS) #Data
# Train and test sets
train <- Pima.tr
test <- Pima.te

head(train)
summary(train)
help(Pima.tr)
```
El campo target es *type* (que es un factor), que indica si el paciente tiene diabetes o no.
```{r}
#Since the type column was a factor, R will encode it to 1 and 2, but this is not what you want: ideally, you would like to work with the type encoded as 0 and 1, which are "No" and "Yes", respectively. In the above code chunk, you subtract 1 from the whole set to get your 0-1 encoding. R will also encode this in the factor order.
y <- as.numeric(train[,8]) - 1
ytest <- as.numeric(test[,8]) - 1

#Predictores y respuestas
x <- data.frame(train[,1:7])
xtest <- data.frame(test[,1:7])

#MODELOS DISPONIBLES:
listWrappers()

#Modelos, usando cross-validation (con 5 intentos):
set.seed(150)
model <- SuperLearner(y, 
                      x,
                      SL.library=list("SL.ranger",
                                      "SL.ksvm",
                                      "SL.ipredbagg",
                                      "SL.bayesglm"))
# Return the model
summary(model)

```

Predicciones con SuperLearner:
```{r}
predictions <- predict.SuperLearner(model, newdata = xtest)

#the overall ensemble predictions
head(predictions$pred)

#the individual library predictions:
head(predictions$library.predict)


#En el caso de clasificación binomial, debemos fijar un umbral (por ejemplo 0.5) y ver los que caen en cada lado
library(dplyr)
conv.preds <- ifelse(predictions$pred >= 0.5,1,0)
library(caret)
#cm <- confusionMatrix(data =conv.preds, reference = ytest)
#cm # Se obtiene  0.7921687 accuracy
```
### Otro ejemplo MUY completo con SUPERLEARNER:
Link: <https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html>




