---
title: "MI ESTUDIO SOBRE R"
output: html_notebook
---

### Generalidades sobre R, RStudio y demás
Link: <https://rpubs.com/bjpredo/lab_00>



### Valores NA (missing values) y cómo rellenarlos
Partimos de uno de los datasets de pruebas que hay en RStudio.
```{r}
#library(help="datasets") #lista completa (cogeremos el dataset llamado attenu, por ejemplo)
data(attenu)
attach(attenu)
help(attenu)
boxplot(attenu, cex.axis=0.5) 
```
Vemos que la variable dist es asimetrica y tiene outliers.
Vamos que las variables mag y accel son leptocúrticas (apretadas en torno a la media)

```{r}
summary(attenu)
```




### Matriz de parejas de features (CORRELACIONES)
Hacemos una análisis exploratorio de las correlaciones.
Matriz de correlaciones (entre variables) y funcion de densidad de probabilidad (univariable):
```{r}
#library(graphics)
#scatterplotMatrix(x=attenu)
```
O también construirla usando una función personalizada por nosotros:
```{r}
pairs(attenu, gap=0, lower.panel = panel.smooth, upper.panel = function(x,y){
  panel.smooth(x,y)
  par(usr=c(0,1,0,1))
  correlacion <- cor(x,y,use="complete.obs")
  text(0.6,0.7,col="blue", cex=1.2, round(correlacion, digits=2))
}  )
```
Es importante ver si hay alguna correlación fácilmente visible (lineal...).

IDs de los elementos que tienen valores NA en cualquiera de sus features (columnas):
```{r}
which(is.na(attenu$event))
which(is.na(attenu$mag))
which(is.na(attenu$station))
which(is.na(attenu$dist))
which(is.na(attenu$accel))
```
Vemos que sólo la variable station tiene valores NA.
También se pueden coger sólo las filas en las que todas las features están rellenas (sin ningún valor NA):
```{r}
attenu[complete.cases(attenu),]
```
Si queremos ELIMINAR aquellas filas (margin=1 significa ROWS) que tengan más de un valor NA:
1. Aplicar la función SUM sobre filas, contando el número de elementos NA.
2. Si hay uno o más, saco sus índices (which)
3. Creo un vector con esos índices y se lo resto al dataset attenu.

```{r}
filas_con_na <- apply(attenu, MARGIN = 1, FUN = function(x){ sum(is.na(x)) >=1})
indices_de_filas_con_na <- which(filas_con_na)
datos_sinhuecos=attenu[ -c( indices_de_filas_con_na ) , ]
```

Comprobamos que ya no hay HUECOS (con algún valor NA):
```{r}
attenu[!complete.cases(datos_sinhuecos),]
```
### ANALISIS DE COMPONENTES PRINCIPALES (PCA)
Si tenemos muchas features, conviene reducirlas para que el análisis de correlaciones sea más sencillo.
Es MUY importante indicar que trabajamos con la matriz de correlaciones (en vez de la de covarianzas): cor=TRUE
Además, PCA sólo trabaja con features que sean NUMERICAS, así que debemos comprobar que todas las columnas (features) son numericas
```{r}
drops <- c("station")
datos_para_pca <- attenu[ , !(names(attenu) %in% drops)]
datos_sin_na <- na.omit(datos_para_pca) # limpiar los NA
sapply(datos_sin_na, class) #clase/modo de cada feature
sapply(datos_sin_na, typeof) #tipo de cada feature
pca_attenu <- princomp(datos_sin_na, cor=TRUE)  
pca_attenu
plot(pca_attenu)
summary(pca_attenu)
biplot(pca_attenu) #PC2 vs PC1

#Los pesos de cada individuo (fila) proyectado en las componentes (PC1, PC2...)
pca_pesos <- pca_attenu$scores
pca_pesos_pc1pc2 <- pca_pesos[, 1:2] #Pesos sólo en las componentes que más influyen en la varianza total
plot(pca_pesos_pc1pc2[,1], pca_pesos_pc1pc2[,2]) #misma gráfica que veiamos en el biplot
```
Vemos que la componente 1 tiene un peso del 54.41% sobre las varianzas; la 2 tiene 26.9%; etc.
Cogemos la PC1 y PC2, que suman un 81% de la influencia en la varianza total.

Vemos también los pesos que tienen las variables de entrada (features) dentro de las componentes compuestas (PC1, PC2...)., mirando el plot de **flechas**:

- PC1 (eje horizontal) está afectada por las variables: event, dist, mag y accel. (es decir, por todas las features de entrada, pero en proporción diferente).
- PC2 (eje vertical): está afectada por mag y accel, principalmente.


### Reduccion de dimensiones, mediante Ensembling (alternativa a PCA)
Tengo muchos features y quiero reducirlos a k.

```{r}
library(tsne)

colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)

ecb <- function(x,y){
  plot(x,t='n');
  text(x,labels=iris$Species, col=colors[iris$Species])
  }

# Tabla inicial: 4 columnas ==> Tabla final: 2 columnas

tsne_iris = tsne(X=iris[,1:4], initial_config = NULL, k = 2, initial_dims = 30, perplexity = 100,
                 max_iter = 1000, min_cost = 0, epoch_callback = ecb, whiten = TRUE,
                 epoch=500)


print('Comparandolo con PCA...')
dev.new()
pca_iris = princomp(iris[,1:4])$scores[,1:2]
plot(pca_iris, t='n')
text(pca_iris, labels=iris$Species,col=colors[iris$Species])
```
Usando un wrapper por encima: <https://cran.r-project.org/web/packages/Rtsne/index.html>
```{r}
# iris_unique <- unique(iris) # Remove duplicates
# iris_matrix <- as.matrix(iris_unique[,1:4])
# set.seed(42) # Set a seed if you want reproducible results
# tsne_out <- Rtsne(iris_matrix) # Run TSNE
# # Show the objects in the 2D tsne representation
# plot(tsne_out$Y,col=iris_unique$Species)
# # Using a dist object
# tsne_out <- Rtsne(dist(iris_matrix))
# plot(tsne_out$Y,col=iris_unique$Species)
# # Use a given initialization of the locations of the points
# tsne_part1 <- Rtsne(iris_unique[,1:4], theta=0.0, pca=FALSE,max_iter=350)
# tsne_part2 <- Rtsne(iris_unique[,1:4], theta=0.0, pca=FALSE, max_iter=150,Y_init=tsne_part1$Y)
```




### CLUSTERING (habiendo hecho PCA antes)
Vamos a mostrar un subtipo de "clustering jerárquico" llamado *Agglomerative Nesting (AGNES)*.

Hay tres tipos de métodos de unión en clustering:

- *Single linkage*: según la MINIMA distancia entre dos elementos de clusters diferentes.
- *Complete linkage*: según la MAXIMA distancia entre dos elementos de clusters diferentes.
- *Average linkage*: según la distancia MEDIA entre dos elementos de clusters diferentes.

```{r}
library(cluster)
agnes_single <- agnes(x = datos_sin_na, method = "single")
plot(agnes_single) # Cuando veamos espacios entre las muestras, es la separacion entre clusters. Es DIFICIL de ver.
```
Con Single, vemos que hay 3 clusters (si height=50), fijándose en los huecos que hay en el gráfico "banner", que separan a los clusters.
```{r}
library(cluster)
agnes_complete <- agnes(x = datos_sin_na, method = "complete")
plot(agnes_complete) # Cuando veamos espacios entre las muestras, es la separacion entre clusters. Es DIFICIL de ver.
```
Con Complete, se ven claramente 3 clusters (si height=200).
```{r}
library(cluster)
agnes_average <- agnes(x = datos_sin_na, method = "average")
plot(agnes_average) # Cuando veamos espacios entre las muestras, es la separacion entre clusters. Es DIFICIL de ver.
```
Con Average, se ven 3 clusters (si height=100).
Donde más claro lo vemos es con el método "complete", así que creamos los *clusters* vistos con *COLORES*.
Esos colores los pintamos sobre los pesos que ya habiamos calculado para PCA (de las componentes PC1 y PC2).
```{r}
complete_3_clusters=cutree(agnes_complete,3)
plot(pca_pesos,col=complete_3_clusters)
```



### REGRESIÓN LINEAL SIMPLE
Link: <http://rpubs.com/joser/RegresionSimple>
Datos entrada:
```{r}
grasas_tabla <- read.table('http://verso.mat.uam.es/~joser.berrendero/datos/EdadPesoGrasas.txt', header = TRUE)
names(grasas_tabla)
pairs(grasas_tabla)
cor(grasas_tabla)
```
Vemos correlación (0.83) entre las variables edad y grasas, como era de esperar a simple vista. El gráfico muestra también la relación lineal (a ojo).

Recta de mínimos cuadrados:
Usamos un MODELO LINEAL (lm) para calcular una regresión lineal muy sencilla, de grasas respecto de la edad: $y =102.575+5.321*x$

O mejor aun:  $y = 102.575 (+/-29.6376) + 5.321(+/-0.7243) * x$

```{r}
regresion <- lm(grasas ~ edad, data = grasas_tabla) #grasas dependiente de la edad
summary(regresion)

plot(grasas_tabla$edad, grasas_tabla$grasas, xlab='Edad', ylab='Grasas')
abline(regresion)

plot(regresion)
```
Vemos que el R^2^ es 0.7012 --> Bondad de la recta de ajuste.

#### PREDICCION PARA NUEVAS ENTRADAS:
Usar la recta:
```{r}
nuevas.edades <- data.frame(edad = seq(30, 50))
prediccion_nueva<-predict(regresion, nuevas.edades)
prediccion_nueva
```

#### Explicación del modelo de regresión simple:
- La columna t-value es el estadístico t --> Cociente entre cada estimador y su error típico. Permite realizar los contrastes de hipótesis nula (que sean 0) --> H~0~:β~0~=0 y H~0~:β~1~=0 
- Los  p-valores aparecen en la columna Pr(>|t|) --> Probabilidad de superar el umbral --> Como son muy pequeños (menores que 0.01 ó 0.05, utilizados normalmente) ==> Se rechazan ambas hipótesis nulas.
- El estimador de la desviación típica (σ) de los errores aparece como Residual standard error y su valor en el ejemplo es 43.5
- Los intervalos de confianza para los parámetros se obtienen con el comando **confint**. El parámetro level permite elegir el nivel de confianza (por defecto es 0.95):
```{r}
confint(regresion)
confint(regresion, level = 0.90)
```

Los "intervalos de confianza para la respuesta media" y los "intervalos de predicción para la respuesta" se pueden obtener usando el comando predict. 
Por ejemplo, el siguiente código calcula y representa los dos tipos de intervalos para el rango de edades que va de 20 a 60 años (los de predicción en rojo):

```{r}
nuevas.edades <- data.frame(edad = seq(20, 60))
# Grafico de dispersion y recta
plot(grasas_tabla$edad, grasas_tabla$grasas, xlab='Edad', ylab='Grasas')
abline(regresion)

# Intervalos de confianza de la respuesta media:
# ic es una matriz con tres columnas: la primera es la prediccion, las otras dos son los extremos del intervalo
ic <- predict(regresion, nuevas.edades, interval = 'confidence')
lines(nuevas.edades$edad, ic[, 2], lty = 2, col = 'green')
lines(nuevas.edades$edad, ic[, 3], lty = 2, col = 'green')

# Intervalos de prediccion
ic <- predict(regresion, nuevas.edades, interval = 'prediction')
lines(nuevas.edades$edad, ic[, 2], lty = 2, col = 'red')
lines(nuevas.edades$edad, ic[, 3], lty = 2, col = 'red')
```
#### Análisis de la varianza:
```{r}
anova(regresion)
```

Diagnóstico del modelo:
```{r}
residuos <- rstandard(regresion)
valores.ajustados <- fitted(regresion)
plot(valores.ajustados, residuos)

#Prueba de normalidad (sencilla) para saber si la muestra tiene una distribucion NORMAL
qqnorm(residuos)
qqline(residuos)
```
Como tiene pocas muestras, podríamos haber pintado una t-Student encima, para poder comparar.

Otras pruebas de normalidad --> Kolmogorov-Smirnov  y Shapiro-Wilk.

#### Ejemplo interesante: 
```{r}
x <- 1:20
w <- 1 + sqrt(x)/2 #desviaciones estandar (pesos)
y <- x + w*rnorm(x)

dummy <-- data.frame(x=x, y=y)

fm <- lm(y~x, data=dummy) #MODELO simple linear regression
summary(fm)

fm1 <- lm(y~x, data=dummy, weight=1/w^2) #MODELO con weighted regression
summary(fm1)
```
Resto del análisis:
```{r}
attach(dummy) #hacemos visibles las columnas del dataframe, para trabajar directamente con sus nombres.
lrf <-lowess(x,y, f = 2/3, iter = 3, delta = 0.01 * diff(range(x))) #Funcion de regresion local no parametrica: LOWESS smoother (locally-weighted polynomial regression)
summary(lrf)
plot(x,y)
lines(x, lrf$y, col="blue") #pintamos la linea de la REGRESION LINEAL

abline(0,1, lty=3, col="magenta") #linea de regresion verdadera (intercept 0, slope=1) (linea de puntos)
abline(coef(fm), col="green") #linea de regresion sin pesos
abline(coef(fm1), col="red") #linea de regresion con pesos

detach()

plot( fitted(fm), resid(fm), xlab="Fitted values", ylab="Residuals", main="Residuals vs Fitted" ) #Analisis de heterodicidad
qqnorm( resid(fm), main="Residuals Rankit Plot" ) #Analisis de skewness, kurtosis y outliers (no muy util en este caso)
#rm( fm, fm1, lrf, x, dummy) #limpieza de variables
```
## Manual de CRAN-INTRO.ap.8.2
Lo básico:
```{r}
attach(faithful)

summary(eruptions) #Como he hecho attach, evito hacer esto: summary(faithful$eruptions)
fivenum(eruptions) #Pinta un "dibujo" hecho con numeros, que parece un histograma
stem(eruptions)
```
Dibujos:
```{r}
hist(eruptions,seq(from=1.6, to=5.2, by=0.2), probability = TRUE)
lines(density(eruptions, bw=0.1))
rug(eruptions)
```
La función de densidad (CDF) es:
```{r}
plot(ecdf(eruptions), do.points=FALSE, verticals = TRUE)
```
Si cogemos sólo aquellos con X>3:
```{r}
erupciones_largas<-eruptions[eruptions>3]
hist(erupciones_largas)
plot(ecdf(erupciones_largas), do.points=FALSE, verticals = TRUE)
x<-seq(3, 5.4, 0.01)

#Usando la distribucion NORMAL (norm)
lines(x, pnorm(x, mean = mean(erupciones_largas), sd=sqrt(var(erupciones_largas))), lty=3)  #linea de puntos(lty=3)
```

### PRUEBAS DE NORMALIDAD:

1. Q-Q (Quantile-Quantile)
```{r}
par(pty="s") #pinta la caja cuadrada (en vez de que sea rectangular por defecto)
qqnorm(erupciones_largas)
qqline(erupciones_largas)

#La COMPARAMOS con una t-Student de 5 grados de libertad:
puntos_t_student <- rt(250, df = 5)
qqnorm(puntos_t_student)
qqline(puntos_t_student)

#Pintamos el qqplot
qqplot(qt( ppoints(250), df=5 ), puntos_t_student, xlab = "QQplot para la densidad t-Student")
qqline(puntos_t_student)
```

2. Test Shapiro-Wilk:
```{r}
shapiro.test(erupciones_largas)
```

3. Test Kolmogorov-Smirnov:
```{r}
ks.test(erupciones_largas, "pnorm", mean=mean(erupciones_largas), sd=sqrt(var(erupciones_largas)))
```


### REGRESION LOGISTICA

```{r}
challenger <- read.table('http://verso.mat.uam.es/~joser.berrendero/datos/challenger.txt', header = TRUE)
table(challenger$defecto)

colores <- NULL
colores[challenger$defecto==0] <- 'green'
colores[challenger$defecto==1] <- 'red'
plot(challenger$temp, challenger$defecto, pch = 21, bg = colores, xlab = 'Temperatura', ylab = 'Probabilidad de defectos')
legend('bottomleft', c('No defecto', 'Si defecto'), pch = 21, col = c('green', 'red'))
```


### DISTRIBUCIONES DE PROBABILIDAD
Hay muchas: binomial (binom), Poisson (pois), normal (norm), exponencial (exp), t-Student (t), chi-cuadrado (chisq), F (f)...


### MAQUINAS DE VECTOR SOPORTE (SVM)
Busca el hiperplano que separe las observaciones según su clases (clasificación).
Buena explicación: <https://www.youtube.com/watch?v=ImhjKyc88x0>

Link de mi ejemplo: <http://rpubs.com/joser/svm>
```{r}
library(MASS)
library(e1071)

prueba_svm <- function() {

load(url('http://www.uam.es/joser.berrendero/datos/practica-svm-io.RData'))

# Mostramos 10 filas
head(breast.cancer2)


#-------- Gráficamente, pintamos la correlación entre las 2 features de entrada: -------
# Prepara los datos
x <- cbind(breast.cancer2$x.smoothness, breast.cancer2$x.concavepoints)
head(x)
y <- breast.cancer2$y
head(y)
n0 <- sum(y==0)
n1 <- sum(y==1)
# Para que los graficos queden mas bonitos (rojo = maligno, verde = benigno)
colores <- c(rep('green',n0),rep('red',n1))
pchn <- 21

# Diagrama de dispersion
plot(x, pch = pchn, bg = colores, xlab='smoothness', ylab='concavepoints')


print('-------------- SVM lineal -------------------------')
# Siendo la variable **y** un factor, queremos predecir usando el resto de features.
# 
# Para calcular la regla de clasificación SVM lineal con C=10, se usa la función svm del paquete e1071. 
# El primer argumento y~. indica que la variable y (que debe ser necesariamente un factor) se desea predecir en términos del resto de variables del fichero. La sintaxis es similar a la que utilizaríamos para ajustar un modelo lineal o un modelo de regresión logística.
# El segundo argumento indica el fichero en el que están las variables que vamos a usar. 
# El argumento kernel corresponde al núcleo que representa el producto escalar que queremos utilizar. 
# La opción linear corresponde a k(x,y)=x′y. 
# El argumento cost determina la penalización que ponemos a los errores de clasificación. 
# Con el fin de estimar la probabilidad de clasificar erróneamente una observación se puede utilizar validación cruzada, dividiendo la muestra en, por ejemplo, dos partes. Ello se consigue fijando cross=2. 
# Finalmente, scale=FALSE se usa para usar los datos no estandarizados (por defecto, sí se estandarizan).

C <- 10
svm.lineal <- svm(y~., data=breast.cancer2, kernel='linear', cost=C, cross=2, scale=FALSE) #Su type es C-classification porque y es un factor
summary(svm.lineal)

x.svm <- x[svm.lineal$index,]
w <- crossprod(x.svm, svm.lineal$coefs)
w0 <- svm.lineal$rho

plot(x, pch = pchn, bg = colores, xlab='smoothness', ylab='concavepoints')
abline(w0/w[2], -w[1]/w[2], lwd=2, col='blue')

#Estadisticas: precisión y cobertura
table(predict(svm.lineal), breast.cancer2$y, dnn=c("Prediction", "Actual"))
library(caret)
confusionMatrix(breast.cancer2$y, predict(svm.lineal))

# Indica que hay 258 vectores soporte.
# Usando validación cruzada con la muestra dividida en dos partes se estima una probabilidad de acierto en la clasificación de aproximadamente el 88%. Podemos cambiar el parámetro de penalización para ver si estos valores aumentan o disminuyen.


print('-------------------- SVM cuadrático --------------')

svm.cuadratico <- svm(formula = y~., data = breast.cancer2, kernel='polynomial', degree=2, gamma=1, coef0=1, cost=C, cross=10, scale=FALSE)
summary(svm.cuadratico)

#Estadisticas: precisión y cobertura
table(predict(svm.cuadratico), breast.cancer2$y, dnn=c("Prediction", "Actual"))
library(caret)
confusionMatrix(breast.cancer2$y, predict(svm.cuadratico))


print('-------------------- SVM radial -----------------')
svm.radial <- svm(formula = y~., data = breast.cancer2, kernel='radial', degree=2, gamma=1, coef0=1, cost=C, cross=10, scale=FALSE)
summary(svm.radial)

#Estadisticas: precisión y cobertura
table(predict(svm.radial), breast.cancer2$y, dnn=c("Prediction", "Actual"))
library(caret)
confusionMatrix(breast.cancer2$y, predict(svm.radial))

}

# prueba_svm()
```
Se ve que es mejor el kernel LINEAL (grado), pero debemos mirar el *accuracy* y *cobertura* --> Gana CUADRATICO, parece.



### ARBOLES DE DECISIÓN
Link: <https://www.youtube.com/watch?v=mgh4KdbYHv0>
Es un algoritmo de CLASIFICACIÓN supervisada. La variable dependiente predicha puede ser:

- Cuantitativa --> Árbol de REGRESIÓN
- Cualitativa --> Árbol de CLASIFICACIÓN

#### Cargar dataset
```{r}
library(rpart) #Algoritmo para árbol de decisión
library(rpart.plot) #Plot de árbol de decisión
library(C50) #paquete que tiene el dataset de entrada y el algoritmo (rpart)
data(churn) #DATASET de ejemplo sobre CHURN (clientes que abandonan una empresa)
View(churn)
```
#### Preparar set de datos, dividiendo en entrenamiento y test
```{r}
churn_juntos <- rbind(churnTest, churnTrain) #junta filas
head(churn_juntos, n=3L) #FEATURES disponibles (5000x20). Mostramos 3 filas
churn <- churn_juntos[, c(4,7,8,16,19,17,20)] #Usaremos solo algunas columnas (features)
names(churn) # nombres en ingles
names(churn) <- c("Tiene plan internacional", "Minutos al dia", "Llamadas al dia", "Minutos internacionales", "Reclamaciones", "Llamadas internacionales", "Cancelacion") #los renombro a nombres en espanhol

ind <- sample(2, nrow(churn), replace=TRUE, prob=c(0.6, 0.4)) #train (60%) y test (40%)
trainData <- churn[ind==1, ] #train
testData <- churn[ind==2, ] #test
```
#### Crear árbol de decisión
```{r}
ArbolRpart <- rpart(Cancelacion ~ ., method="class", data=trainData) #Formula: variable dependiente (Cancelacion) depende todas las otras
```

#### Gráficos
```{r}
print(ArbolRpart)                         
rpart.plot(ArbolRpart,extra=4)  # extra=4:probabilidad de observaciones por clase

printcp(ArbolRpart)             # estadísticas de resultados
plotcp(ArbolRpart)              # evolución del error a medida que se incrementan los nodos
```

```{r}
# Podado del árbol
pArbolRpart<- prune(ArbolRpart, cp= ArbolRpart$cptable[which.min(ArbolRpart$cptable[,"xerror"]),"CP"])
pArbolRpart<- prune(ArbolRpart, cp= 0.011111)
printcp(pArbolRpart)
```

#### Predice las cancelaciones en testData 
```{r}
# Validamos la capacidad de predicción del árbol con el fichero de validación
testPredRpart <- predict(ArbolRpart, newdata = testData, type = "class")

# Visualizamos una matriz de confusión
table(testPredRpart, testData$Cancelacion)
```

#### Estadística del modelo  
```{r}
# Calculamos el % de aciertos
sum(testPredRpart == testData$Cancelacion) / length(testData$Cancelacion)*100
```


### SVM (otro ejemplo)
Modelo SVM sobre datos de iris:
```{r}
data(iris)
attach(iris)

## classification mode
# default with factor response:
model <- svm(Species ~ ., data = iris)

# alternatively the traditional interface:
x <- subset(iris, select = -Species)
y <- Species
model <- svm(x, y) 

print(model)
summary(model)
```
Testeo con datos de entrenamiento (claro, el modelo está muy ajustado para ellos):
```{r}
# test with train data
pred <- predict(model, x)
# (same as:)
#pred <- fitted(model)

# Check accuracy:
table(pred, y)

# compute decision values and probabilities:
pred <- predict(model, x, decision.values = TRUE)
attr(pred, "decision.values")[1:4,]

# visualize (classes by color, SV by crosses):
distancias_escaladas <- cmdscale(dist(iris[,-5]))
plot(distancias_escaladas,
     col = as.integer(iris[,5]),
     pch = c("o","+")[1:150 %in% model$index + 1])

```
Modelo de REGRESION en 2 dimensiones:
```{r}
## try regression mode on two dimensions

# create data
x <- seq(0.1, 5, by = 0.05)
y <- log(x) + rnorm(x, sd = 0.2)

# estimate model and predict input values
m   <- svm(x, y)
summary(m)
new <- predict(m, x)

# visualize
plot(x, y)
points(x, log(x), col = 2)
points(x, new, col = 4)

## density-estimation

```
Creamos otras features con distribución normal y modelamos:
```{r}
# create 2-dim. normal with rho=0:
X <- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)

# MODELO:
#traditional way:
#m <- svm(X, gamma = 0.1)
# formula interface:
m <- svm(~., data = X, gamma = 0.1)
# Otra forma:X <- data.frame(a = rnorm(1000), b = rnorm(1000))
attach(X)

#m <- svm(~ a + b, gamma = 0.1)

summary(m)

# test:
newdata <- data.frame(a = c(0, 4), b = c(0, 4))
predict(m, newdata)

# visualize:
plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))
points(newdata, pch = "+", col = 2, cex = 5)

# weights: (example not particularly sensible)
i2 <- iris
levels(i2$Species)[3] <- "versicolor"
summary(i2$Species)
wts <- 100 / table(i2$Species)
wts
m <- svm(Species ~ ., data = i2, class.weights = wts)
```


### SVM (ejemplo perros y gatos)
Link: <https://www.youtube.com/watch?v=ImhjKyc88x0>
```{r}
library(e1071)
data(cats, package = 'MASS') # Features: sexo (sex), peso (bwt), peso_corazon (hwt)

#Datasets: train y test
ind <- sample(x=2, size = nrow(cats), replace = TRUE, prob = c(0.7,0.3)) #vector 1:144 de índices. Sus valores están en el rango entre 1:x; en este caso, los unos aparecerán un 70% y los doses un 30%.
testset <- cats[ind == 1,] #Coge las filas de CATS en aquellos indices que 'ind' toma valor 1
trainset <- cats[ind == 2,] #Coge las filas de CATS en aquellos indices que 'ind' toma valor 2

#Entrenamos el modelo
modelo_svm <- svm(Sex~., data=trainset, kernel = "radial")
prediccion <- predict(modelo_svm, newdata = testset[-1])

#Resultado y porcentaje de acierto
plot(modelo_svm, cats)
MC <- table(testset[,1], prediccion) #matriz de confusión
MC
acierto <- ( sum(diag(MC)) )/ (sum(MC))
acierto

```


###Otro ejemplo completo
Link: <https://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package/>

####Data split:
```{r}
# load the libraries
library(caret)
library(klaR)
# load the iris dataset
data(iris)
# define an 80%/20% train/test split of the dataset
split=0.80
trainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)
data_train <- iris[ trainIndex,]
data_test <- iris[-trainIndex,]
# train a naive bayes model
model <- NaiveBayes(Species~., data=data_train)
# make predictions
x_test <- data_test[,1:4]
y_test <- data_test[,5]
predictions <- predict(model, x_test)
# summarize results
confusionMatrix(predictions$class, y_test)
```
####Bootstrap:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="boot", number=100)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```
####k-fold Cross Validation:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")#, tuneGrid=grid
# summarize results
print(model)
```
####Repeated k-fold Cross Validation:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```
####Leave One Out Cross Validation:
```{r}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="LOOCV")
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```

###Otro ejemplo SVM vs RPART:
```{r}
library(e1071)
library(rpart)

#DATOS:
data(Ozone, package = "mlbench")

## Split en Datasets (train y test)
index     <- 1:nrow(Ozone)
tercio <- trunc(length(index)/3) #usamos un 30% para test
testindex <- sample(index, tercio, replace = FALSE, prob = NULL)
testset   <- na.omit(Ozone[testindex,]) #TEST Quita las filas que tengan ALGUN valor NA
trainset  <- na.omit(Ozone[-testindex,])#TRAIN Quita las filas que tengan ALGUN valor NA

#Columna TARGET
indice_target <- which( colnames(trainset) == "V4" )

## svm (MAQUINAS VECTOR SOPORTE) para REGRESIÓN NO LINEAL
svm.model <- svm(formula = V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001)
svm.pred  <- predict(svm.model, testset[,-indice_target])
svm_error <- crossprod(svm.pred - testset[,indice_target]) / length(testindex)
svm_error

## rpart (ARBOLES DE REGRESION) para REGRESIÓN NO LINEAL
rpart.model <- rpart(formula = V4 ~ ., data = trainset)
rpart.pred  <- predict(rpart.model, testset[,-indice_target])
rpart_error <- crossprod(rpart.pred - testset[,indice_target]) / length(testindex)
rpart_error
```

###ENSEMBLING
Link: <https://www.datacamp.com/community/tutorials/ensemble-r-machine-learning>
El ensemble es un supermodelo formado por otros modelos ponderados con pesos segun su riesgo de acierto (calculado internamente al entrenar).
```{r}
library("SuperLearner")

library(MASS) #Data
# Train and test sets
train <- Pima.tr
test <- Pima.te

head(train)
summary(train)
help(Pima.tr)
```
El campo target es *type* (que es un factor), que indica si el paciente tiene diabetes o no.
```{r}
#Since the type column was a factor, R will encode it to 1 and 2, but this is not what you want: ideally, you would like to work with the type encoded as 0 and 1, which are "No" and "Yes", respectively. In the above code chunk, you subtract 1 from the whole set to get your 0-1 encoding. R will also encode this in the factor order.
y <- as.numeric(train[,8]) - 1
ytest <- as.numeric(test[,8]) - 1

#Predictores y respuestas
x <- data.frame(train[,1:7])
xtest <- data.frame(test[,1:7])

#MODELOS DISPONIBLES:
listWrappers()
#vignette("SuperLearner") #Parametros ajustables de esos modelos

#Modelos, usando cross-validation (con 5 intentos):
set.seed(150)
model <- SuperLearner(y, 
                      x,
                      SL.library=list("SL.ranger",
                                      "SL.ksvm",
                                      "SL.ipredbagg",
                                      "SL.bayesglm"))
# Return the model
summary(model)

```

Predicciones con SuperLearner:
```{r}
#Si metemos como nuevos datos X al conjunto xtest:
predictions <- predict.SuperLearner(model, newdata = xtest)

#Respuesta IDEAL (target)
head(ytest, n=10L)

#Respuesta con el supermodelo ENSEMBLE (target predicho por el ENSEMBLE):
head(predictions$pred, n=10L)

#Respuesta calculada por cada modelo individual (los que forman el ensemble):
head(predictions$library.predict, n=10L)

#MEDICION DE LOS ACIERTOS
#En el caso de clasificación binomial, debemos fijar un umbral (por ejemplo 0.5) y ver los que caen en cada lado
library(dplyr)
conv.preds <- ifelse(predictions$pred >= 0.5, 1, 0)
library(caret)
f_conv_preds <- factor(conv.preds)
str(f_conv_preds)
f_ytest <- factor(ytest)
str(f_ytest)
cm <- confusionMatrix(data = f_conv_preds, reference = f_ytest)
cm # Se obtiene  0.7921687 accuracy
```
###Otro ejemplo MUY completo con SUPERLEARNER:
Link: <https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html>

####Monocore vs MULTICORE-Simple vs MULTICORE-CV vs SNOW>: el resultado debe coincidir
Link: <http://benkeser.github.io/sllecture/>
```{r}
oscuridad_pelo <- c(1,2,3,4,5,6,6,6,6,6)
edad <- c(20,25,50,30,26,31,32,33,34,35)
TARGET <- c(15.03, 18.02, 22.41, 17.40, 15.45, 15.22, 14.99, 15.42, 15.88, 16.30) #tiempo de carrera: cuanto más oscuro y menor edad ==> menor tiempo
matrizentrada <- data.frame(oscuridad_pelo, edad, TARGET)
class(matrizentrada)
matrizentrada

#Crear datasets features+target para train y test
index     <- 1:nrow(matrizentrada)
tercio <- trunc(0.30 * length(index)) #70%-TRAIN, 30%-Test
testindex <- sample(index, tercio, replace = FALSE, prob = NULL)
testset   <- na.omit(matrizentrada[testindex,])
trainset  <- na.omit(matrizentrada[-testindex,])

indice_target <- which( colnames(trainset) == "TARGET" ) #Columna TARGET
x_train <- subset(trainset, select = -indice_target)
y_train <- trainset$TARGET
x_test <- subset(testset, select = -indice_target)
y_test <- testset$TARGET
  
print( paste( "matrizentrada=",nrow(matrizentrada), "x", ncol(matrizentrada) ) )
print( paste( "testset=",nrow(testset), "x", ncol(testset) ) )
print( paste( "trainset=",nrow(trainset), "x", ncol(trainset) ) )
print( paste( "x_train=",nrow(x_train), "x", ncol(x_train) ) )
print( paste( "y_train=",length(y_train) ) )
print( paste( "x_test=",nrow(x_test), "x", ncol(x_test) ) )
print( paste( "y_test=",length(y_test) ) )

######################## MODELOS PREDICTIVOS ############3
library("SuperLearner")
library(nnls)
library(nnet)
library(randomForest)
library(ggplot2)
library(parallel)
algoritmosPredictivosUsados <- list("SL.randomForest", "SL.nnet")

print('-------- UNICORE (con cross validation) ----------')
modelo_unicore <- SuperLearner(Y = y_train, X = x_train, family = gaussian(), 
                             SL.library = algoritmosPredictivosUsados, method = "method.NNLS",
                             id = NULL, verbose = FALSE,
                             control = list(), cvControl = list(), obsWeights = NULL, env = parent.frame())
summary(modelo_unicore)
print('Validando modelo UNICORE sobre dataset de test...')
out_unicore <- predict.SuperLearner(object = modelo_unicore, newdata = x_test, onlySL = TRUE) #No usa los que tienen peso =0
print('PREDICCION:')
out_unicore_predicho <- out_unicore$pred; str(out_unicore_predicho)
print('ESPERABLE:')
str(y_test)
qplot(y_test, out_unicore_predicho) #Scatterplot


print('-------- MULTICORE-Simple (con cross validation) ----------')
set.seed(1, "L'Ecuyer-CMRG") #multicore compatible seed
modelo_multicore_sin_cv = mcSuperLearner(Y = y_train, X = x_train, family = gaussian(),
                                         SL.library = algoritmosPredictivosUsados, verbose = FALSE)
summary(modelo_multicore_sin_cv)
print('Validando modelo MULTICORE (sin cross validation) sobre dataset de test...')
out_multicore_simple <- predict.SuperLearner(object = modelo_unicore, newdata = x_test, onlySL = TRUE) #No usa los que tienen peso =0
print('PREDICCION:')
out_multicore_simple_predicho <- out_multicore_simple$pred; str(out_multicore_simple_predicho)
print('ESPERABLE:')
str(y_test)
qplot(y_test, out_multicore_simple_predicho) #Scatterplot


print('-------- MULTICORE-CV (con cross validation) ----------')
print('Se usa para EVALUAR EL RENDIMIENTO: para elegir algoritmos y sus parametros adecuados. Luego habrá que usarlos en un modelo sin CV')
print('Se puede usar la libreria SNOW (Windows, Linux; pero dificil) o multicore (Linux).')
num_cores_disponibles <- RhpcBLASctl::get_num_cores()
num_cores_usados <- (num_cores_disponibles)
print(paste('Uso ', num_cores_usados, ' cores de ', num_cores_disponibles, ' cores disponibles'))
options(mc.cores = num_cores_usados) #Uso todas las CPUs (menos una, para dejar libre el PC para trabajar)
getOption("mc.cores") #En Linux, comprobamos su estamos usando todos los cores

# We need to set a different type of seed that works across cores.
# Otherwise the other cores will go rogue and we won't get repeatable results.
# This version is for the "multicore" parallel system in R.
set.seed(1, "L'Ecuyer-CMRG")

num_v <- 3 # splits the data into V folds and then calls SuperLearner
internal_v <- 2 #inner cross-validation process (replicated across all folds)

print('SUPERLEARNER con MULTICORE:')
print('Con método NNLS (Non-negative Least Squares)')
print(paste('num_v = ', num_v))
print(paste('internal_v = ', internal_v))
print('Esta forma con CV no mide el rendimiento, asi que lo medimos por fuera...')

system.time({
    modelo_multicore_con_cv <- CV.SuperLearner(Y = y_train, X = x_train, family = gaussian(), 
                          parallel = "multicore",
                          SL.library = algoritmosPredictivosUsados, 
                          method = "method.NNLS", verbose = F,
                          cvControl = list(V = num_v, shuffle = FALSE), 
                          innerCvControl = list(list(V = internal_v)), 
                          saveAll = FALSE)
})

library(ggplot2)

print('Modelo:')
print( summary(modelo_multicore_con_cv) )

print('En cada fold, ha ganado este algoritmo:')
table( simplify2array( modelo_multicore_con_cv$whichDiscreteSL ) )
print('En cada fold, cada algoritmo tiene estos pesos:')
modelo_multicore_con_cv$coef
print('Plot the performance with 95% CIs (use a better ggplot theme):')
plot(modelo_multicore_con_cv) + theme_bw()


# Review meta-weights (coefficients) from a CV.SuperLearner object
review_weights <- function(cv_sl) {
  meta_weights = coef(cv_sl)
  means = colMeans(meta_weights)
  sds = apply(meta_weights, MARGIN = 2,  FUN = function(col) { sd(col) })
  mins = apply(meta_weights, MARGIN = 2, FUN = function(col) { min(col) })
  maxs = apply(meta_weights, MARGIN = 2, FUN = function(col) { max(col) })
  # Combine the stats into a single matrix.
  sl_stats = cbind("mean(weight)" = means, "sd" = sds, "min" = mins, "max" = maxs)
  # Sort by decreasing mean weight.
  sl_stats[order(sl_stats[, 1], decreasing = T), ]
}

print(review_weights(modelo_multicore_con_cv), digits = 3)


print('-------- SNOW: procesado multicore (sirve en Linux y Windows)----------')



```

####Prueba de SuperLearner solo con los algoritmos de REGRESION (los indicados por Matlab)
```{r}
# Linear Regression, GLM

# SVR, GPR

# Ensemble methods

# Decision Trees

# Neural Networks


```

### SUBSEMBLE:
Link: <https://www.stat.berkeley.edu/~ledell/docs/dlab_ensembles.pdf>

Se divide la entrada en subsets y después, se aplica un ensemble en cada uno de esos subsets.
```{r}

```

